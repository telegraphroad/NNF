{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/VincentStimper/normalizing-flows/blob/master/example/real_nvp_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZOHzj6Zf3bh"
   },
   "source": [
    "# Illustration of the usage of the `normflows` package\n",
    "## Training a Real NVP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWKHE21nf8ue"
   },
   "source": [
    "This notebook illustrates how to use the `normflows` packages by training a simple Real NVP model to a 2D distribution consisting on two half moons.\n",
    "\n",
    "Before we can start, we have to install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUbS_OlPgXNb",
    "outputId": "b4ce77f5-bc0a-4e70-b1c1-3b70f0d98f97"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/telegraphroad/NNF.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kdLk_paf3bk"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import normflows as nf\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCPX0icbgrEA"
   },
   "source": [
    "After importing the required packages, we want to create a `nf.NormalizingFlow` model. Therefore, we need a base distribution, which we set to be a Gaussian, and a list of flow layers. The flow layers are simply affine coupling layers, whereby `nf.AffineCouplingBlock` already includes the splitting and merging of the features as it is done in coupling. We also swap the features after each layer to ensure that they are all modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta_0PfGqf3bm"
   },
   "outputs": [],
   "source": [
    "# Set up model\n",
    "\n",
    "# Define 2D Gaussian base distribution\n",
    "base = nf.distributions.base.DiagGaussian(2)\n",
    "base = nf.distributions.base.GaussianMixture(n_modes = 10, dim = 2, trainable=True)\n",
    "\n",
    "# Define list of flows\n",
    "num_layers = 32\n",
    "flows = []\n",
    "for i in range(num_layers):\n",
    "    # Neural network with two hidden layers having 64 units each\n",
    "    # Last layer is initialized by zeros making training more stable\n",
    "    param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True)\n",
    "    # Add flow layer\n",
    "    flows.append(nf.flows.AffineCouplingBlock(param_map))\n",
    "    # Swap dimensions\n",
    "    flows.append(nf.flows.Permute(2, mode='swap'))\n",
    "    \n",
    "# Construct flow model\n",
    "model = nf.NormalizingFlow(base, flows)\n",
    "bestmodel = nf.NormalizingFlow(base, flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxKwEBUhf3bm"
   },
   "outputs": [],
   "source": [
    "# Move model on GPU if available\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z191AKvh1KO"
   },
   "source": [
    "This is our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "5hOhv--5f3bn",
    "outputId": "c9e72ca6-53f2-4929-8800-1b0ee7077eac"
   },
   "outputs": [],
   "source": [
    "# # Plot target distribution\n",
    "# x_np, _ = make_moons(2 ** 20, noise=0.1)\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.hist2d(x_np[:, 0], x_np[:, 1], bins=200, range=[[-1.5, 2.5], [-2, 2]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "K1umY7b2f3bo",
    "outputId": "03363ac7-9b22-4056-f6f1-e9b7a9041e7e"
   },
   "outputs": [],
   "source": [
    "# # Plot initial flow distribution\n",
    "# grid_size = 200\n",
    "# xx, yy = torch.meshgrid(torch.linspace(-1.5, 2.5, grid_size), torch.linspace(-2, 2, grid_size))\n",
    "# zz = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\n",
    "# zz = zz.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# log_prob = model.log_prob(zz).to('cpu').view(*xx.shape)\n",
    "# model.train()\n",
    "# prob = torch.exp(log_prob)\n",
    "# prob[torch.isnan(prob)] = 0\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.pcolormesh(xx, yy, prob.data.numpy())\n",
    "# plt.gca().set_aspect('equal', 'box')\n",
    "# plt.show()\n",
    "import torch.distributions as D\n",
    "mix = D.Categorical(torch.tensor([1.,.2]))\n",
    "m = 5.\n",
    "m2 = 8.\n",
    "v1 = .9\n",
    "v2 = .9\n",
    "v3 = .5\n",
    "v4 = .5\n",
    "comp = D.Independent(D.Normal(\n",
    "             torch.tensor([[m,m],[m2,m2]]), torch.tensor([[v1,v2],[v3,v4]])), 1)\n",
    "gmm = D.MixtureSameFamily(mix, comp)\n",
    "\n",
    "smpl = gmm.sample([10000])\n",
    "colors = gmm.log_prob(smpl).exp()\n",
    "smpl = smpl.detach().cpu().numpy()\n",
    "colors = colors.detach().cpu().numpy()\n",
    "plt.scatter(smpl[:,0],smpl[:,1],c=colors)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXkqF_W0h6FT"
   },
   "source": [
    "Now, we are ready to train the flow model. This can be done in a similar fashion as standard neural networks. Since we use samples from the target for training, we use the forward KL divergence as objective, which is equivalent to maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WC8o3MdCf3bp",
    "outputId": "2cf1a2f0-0833-4e10-9960-aac79087bce5"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "max_iter = 10000\n",
    "num_samples = 2 ** 11\n",
    "show_iter = 500\n",
    "\n",
    "\n",
    "loss_hist = np.array([])\n",
    "bestloss = 9999999999999999999999999.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
    "checkpoints = []\n",
    "for it in tqdm(range(max_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get training samples\n",
    "    x_np = gmm.sample([2048]).detach().cpu().numpy()\n",
    "    x = torch.tensor(x_np).float().to(device)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = model.forward_kld(x)\n",
    "    \n",
    "    # Do backprop and optimizer step\n",
    "    if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Log loss\n",
    "    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "    if loss.detach().cpu().item() < bestloss:\n",
    "        bestmodel.state_dict = model.state_dict\n",
    "        bestloss = loss.detach().cpu().item()\n",
    "    # Plot learned posterior\n",
    "    if (it + 1) % show_iter == 0:\n",
    "        checkpoints.append(model.state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        # plt.figure(figsize=(15, 15)) \n",
    "        # plt.pcolormesh(xx, yy, prob.data.numpy())\n",
    "        # plt.gca().set_aspect('equal', 'box')\n",
    "        # plt.show()\n",
    "        plt.plot()\n",
    "        fig, ax = plt.subplots(1,2,figsize=[25,10])\n",
    "        a = model.sample(10000)[0].detach().cpu().numpy()\n",
    "        colors = model.log_prob(torch.tensor(a).cuda()).exp().detach().cpu().numpy()  \n",
    "\n",
    "        ax[0].scatter(a[:,0],a[:,1],c=colors)\n",
    "        \n",
    "        \n",
    "\n",
    "        m2 = D.Normal(loc=torch.tensor([8.,8.]),scale=torch.tensor([0.5,0.5]))\n",
    "        smpl = m2.sample([1000]).cuda()\n",
    "\n",
    "        z,l = bestmodel.forward_kld(smpl,extended=True)\n",
    "        z = z.cpu().detach().numpy()\n",
    "        \n",
    "        ax[1].scatter(z[:,0],z[:,1])\n",
    "        means = model.q0.loc.squeeze().cpu().detach().numpy()\n",
    "        covs = model.q0.log_scale.exp().squeeze().cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        ax[1].scatter(means[:,0],means[:,1],color='red')\n",
    "        ax[1].set_title('Samples from \"tail area\" mapped to latent space')\n",
    "        for i, txt in enumerate(means[:,0]):\n",
    "            ax[1].annotate(txt.round(decimals=4), (means[i,0], means[i,1]))\n",
    "        \n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(loss_hist, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycFsV2y3kQnt"
   },
   "source": [
    "This is our trained flow model!\n",
    "\n",
    "Note that there might be a density filament connecting the two modes, which is due to an architectural limitation of normalizing flows, especially prominent in Real NVP. You can find out more about it in [this paper](https://proceedings.mlr.press/v151/stimper22a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bestmodel.sample(20000)[0].detach().cpu().numpy()\n",
    "colors = bestmodel.log_prob(torch.tensor(a).cuda()).exp().detach().cpu().numpy()  \n",
    "\n",
    "plt.scatter(a[:,0],a[:,1],c=colors)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = D.Normal(loc=torch.tensor([8.,8.]),scale=torch.tensor([0.5,0.5]))\n",
    "smpl = torch.tensor(m2.sample([1000])).cuda()\n",
    "\n",
    "z,l = bestmodel.forward_kld(smpl,extended=True)\n",
    "z = z.cpu().detach().numpy()\n",
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "ax.scatter(z[:,0],z[:,1])\n",
    "means = model.q0.loc.squeeze().cpu().detach().numpy()\n",
    "covs = model.q0.log_scale.exp().squeeze().cpu().detach().numpy()\n",
    "\n",
    "\n",
    "ax.scatter(means[:,0],means[:,1],color='red')\n",
    "ax.set_title('Samples from \"tail area\" mapped to latent space')\n",
    "for i, txt in enumerate(means[:,0]):\n",
    "    ax.annotate(txt.round(decimals=4), (means[i,0], means[i,1]))\n",
    "means\n",
    "covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = D.MultivariateNormal(torch.tensor([0.89982155,  0.8713162]),torch.tensor([[1.02290353,0],[0, 1.07131946]]))\n",
    "plt.scatter(a[:,0],a[:,1],c=colors)\n",
    "\n",
    "smpl = torch.tensor(m2.sample([1000])).float().cuda()\n",
    "lp = m2.log_prob(smpl.cpu()).cuda()\n",
    "print(lp.shape)\n",
    "#bestmodel.q0 = m2\n",
    "z= model.backward(1000,z=smpl,log_q_=lp).cpu().detach().numpy()\n",
    "#z= model.backward(1000).cpu().detach().numpy()\n",
    "plt.scatter(z[:,0],z[:,1])\n",
    "m2 = D.MultivariateNormal(torch.tensor([1.59810422,  0.98684428]),torch.tensor([[0.94473827,0],[0, 0.95012178]]))\n",
    "\n",
    "smpl = torch.tensor(m2.sample([1000])).float().cuda()\n",
    "lp = m2.log_prob(smpl.cpu()).cuda()\n",
    "print(lp.shape)\n",
    "#bestmodel.q0 = m2\n",
    "z= model.backward(1000,z=smpl,log_q_=lp).cpu().detach().numpy()\n",
    "#z= model.backward(1000).cpu().detach().numpy()\n",
    "plt.scatter(z[:,0],z[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "G5ZcFoG-f3bq",
    "outputId": "3883a118-1183-49c8-8a7e-6808908b7045"
   },
   "outputs": [],
   "source": [
    "# Plot learned posterior distribution\n",
    "model.eval()\n",
    "log_prob = model.log_prob(zz).to('cpu').view(*xx.shape)\n",
    "model.train()\n",
    "prob = torch.exp(log_prob)\n",
    "prob[torch.isnan(prob)] = 0\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.pcolormesh(xx, yy, prob.data.numpy())\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "real_nvp_colab.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
